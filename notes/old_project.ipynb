{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Enhancing Visual Reasoning in VQA Tasks through Interactive Learning\n",
    "\n",
    "For my course project, I want to explore if interactive learning can improve spatial reasoning in Visual Question Answering (VQA) tasks.\n",
    "\n",
    "## Background\n",
    "\n",
    "From our readings and discussions so far, one of the key observations I've made is that current neural network models struggle with tasks requiring spatial reasoning or counting. They often rely on **pattern recognition** rather than true spatial understanding. Given this, I am somewhat skeptical of the research path based on the inherent, restrictive approach and am more drawn to exploring **interactive learning**. Can interactive learning bridge this gap, or bring more insights to traditional models?\n",
    "\n",
    "## Available Resources\n",
    "\n",
    "**Dataset**: CLEVR, a diagnostic dataset for testing spatial and logical reasoning. It also provides scene graphs as a foundation of interactive learning.  \n",
    "**Existing Baselines**: Neural network-based VQA models (e.g., CNN + LSTM).\n",
    "\n",
    "## Current Approaches\n",
    "\n",
    "1. Implement a baseline VQA model to evaluate counting and spatial reasoning tasks. \n",
    "2. Introduce online learning approach, using given answers and programs to iteratively improve model predictions.\n",
    "Compare model performance across these approaches and analyze their reasoning capabilities.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "Improved accuracy on counting and spatial reasoning tasks through interaction.\n",
    "Insights into the potential interactive methods for improving static learning in VQA systems.\n",
    "\n",
    "CLEVR paper: https://arxiv.org/pdf/1612.06890\n",
    "\n",
    "dataset: https://github.com/facebookresearch/clevr-dataset-gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Data\n",
    "In preliminary practice, I control the length of program to limit the complexity of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/srv/data/CLEVR_v1.0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRDataset(Dataset):\n",
    "    def __init__(self, corpus_dir, split='train', transform=None, max_program_length=None):\n",
    "        \"\"\"\n",
    "        Initialize CLEVR dataset\n",
    "        :param corpus_dir: root directory path of CLEVR dataset\n",
    "        :param split: data split ('train', 'val', 'test')\n",
    "        :param transform: image preprocessing transform\n",
    "        :param max_program_length: maximum length of program, subset questions\n",
    "        \"\"\"\n",
    "        self.path = Path(corpus_dir)\n",
    "        self.split = split\n",
    "        # self.mode = mode  # 'train' or 'test'\n",
    "        # self.feedback_mode = feedback_mode  # 'none', 'answer', 'program'\n",
    "        self.transform = transform \n",
    "        self.max_program_length = max_program_length\n",
    "\n",
    "        questions_file = self.path / f'questions/CLEVR_{self.split}_questions.json'\n",
    "        all_items = json.load(questions_file.open())['questions']\n",
    "\n",
    "        # subset\n",
    "        if self.max_program_length is not None:\n",
    "            self.items = [\n",
    "                item for item in all_items if len(item.get('program', [])) <= self.max_program_length\n",
    "            ]\n",
    "        else:\n",
    "            self.items = all_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "\n",
    "        # load image\n",
    "        img_path = self.path / 'images' / self.split / item['image_filename']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # basic data\n",
    "        data = {\n",
    "            'image': image, \n",
    "            'question': item['question'], \n",
    "            'answer': item.get('answer', '<NO_ANS>'), \n",
    "            'program': item.get('program', [])  # optional\n",
    "        }\n",
    "\n",
    "        # # add feedback \n",
    "        # if self.mode == 'train' and self.feedback_mode != 'none':\n",
    "        #     if self.feedback_mode == 'answer':\n",
    "        #         data['feedback'] = {'answer': item.get('answer', '<NO_ANS>')}\n",
    "        #     elif self.feedback_mode == 'program':\n",
    "        #         data['feedback'] = {'program': item.get('program', [])}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load scene graphs\n",
    "\n",
    "# scene_path = folder_path + \"/scenes/CLEVR_val_scenes.json\"\n",
    "# with open(scene_path, 'r') as f:\n",
    "#     scenes = json.load(f)\n",
    "\n",
    "# # look into the structure\n",
    "# print(scenes['scenes'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load questions\n",
    "\n",
    "# questions_path = folder_path + \"/questions/CLEVR_val_questions.json\"\n",
    "# with open(questions_path, 'r') as f:\n",
    "#     questions = json.load(f)\n",
    "\n",
    "# # # look into the structure\n",
    "# # print(questions['questions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect simple 'count' questions as a starting point\n",
    "# def extract_count_questions(questions_data, max_steps=3):\n",
    "#     count_questions = []\n",
    "#     for question in questions_data['questions']:\n",
    "#         program = question['program']\n",
    "#         if any(step['function'] == 'count' for step in program) and len(program) <= max_steps:\n",
    "#             count_questions.append({\n",
    "#                 'question_index': question['question_index'],\n",
    "#                 'image_index': question['image_index'],\n",
    "#                 'image_filename': question['image_filename'],\n",
    "#                 'question': question['question'],\n",
    "#                 'answer': question['answer'],\n",
    "#                 'program': question['program']\n",
    "#             })\n",
    "#     return count_questions\n",
    "\n",
    "# count_questions = extract_count_questions(questions, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Total 'count' questions found: {len(count_questions)}\")\n",
    "# print(\"Sample 'count' question:\", count_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate the problem and analyze its program\n",
    "# for question in count_questions:\n",
    "#     print(\"Question:\", question['question'])\n",
    "#     print(\"Answer:\", question['answer'])\n",
    "#     print(\"Program:\")\n",
    "    \n",
    "#     for step in question['program']:\n",
    "#         print(f\"  Function: {step['function']}, Inputs: {step['inputs']}, Values: {step['value_inputs']}\")\n",
    "#     print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(count_questions)\n",
    "\n",
    "# # Split data for initialization and testing\n",
    "# train_questions = count_questions[:200]\n",
    "# test_questions = count_questions[200:262]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline VQA Model\n",
    "\n",
    "In previous lecture, Bill introduced a base VQA model, which utilized LSTM model to process question and VGG16 to extract image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CLEVRDataset(folder_path, split='train', max_program_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a global vocab\n",
    "def load_all_questions(folder_path):\n",
    "    all_items = []\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        questions_file = Path(folder_path) / f'questions/CLEVR_{split}_questions.json'\n",
    "        all_items.extend(json.load(questions_file.open())['questions'])\n",
    "    return all_items\n",
    "\n",
    "tokenize = lambda text: re.findall(r'\\w+|\\S', text.lower())\n",
    "def build_global_vocab(folder_path):\n",
    "    all_items = load_all_questions(folder_path)\n",
    "\n",
    "    # debug\n",
    "    for item in all_items:\n",
    "        if 'answer' not in item:\n",
    "            print(\"Item without 'answer' key:\", item)\n",
    "            break\n",
    "            \n",
    "    all_questions = [tokenize(item['question']) for item in all_items if 'question' in item]\n",
    "    all_answers = [item.get('answer', '<NO_ANS>') for item in all_items]\n",
    "\n",
    "    question_vocab = Counter([token for question in all_questions for token in question])\n",
    "    answer_vocab = Counter(all_answers)\n",
    "\n",
    "    itos_q = ['<PAD>', '<UNK>'] + [s[0] for s in question_vocab.most_common()]\n",
    "    stoi_q = {s: i for i, s in enumerate(itos_q)}\n",
    "    pad_idx, unk_idx = stoi_q['<PAD>'], stoi_q['<UNK>']\n",
    "\n",
    "    itos_a = ['<NO_ANS>'] + [s[0] for s in answer_vocab.most_common()]\n",
    "    stoi_a = {s: i for i, s in enumerate(itos_a)}\n",
    "\n",
    "    return question_vocab, answer_vocab, stoi_q, itos_q, stoi_a, itos_a, pad_idx, unk_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item without 'answer' key: {'image_index': 0, 'split': 'test', 'image_filename': 'CLEVR_test_000000.png', 'question_index': 0, 'question': 'Is there anything else that is the same shape as the small brown matte object?'}\n"
     ]
    }
   ],
   "source": [
    "question_vocab, answer_vocab, stoi_q, itos_q, stoi_a, itos_a, pad_idx, unk_idx = build_global_vocab(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'no': 174753,\n",
       "         'yes': 168916,\n",
       "         '<NO_ANS>': 149988,\n",
       "         '1': 70220,\n",
       "         '0': 59178,\n",
       "         'small': 38426,\n",
       "         'rubber': 38134,\n",
       "         'metal': 38100,\n",
       "         'large': 38016,\n",
       "         '2': 36155,\n",
       "         'sphere': 25491,\n",
       "         'cube': 25476,\n",
       "         'cylinder': 25458,\n",
       "         '3': 18624,\n",
       "         'purple': 9674,\n",
       "         'yellow': 9671,\n",
       "         'blue': 9596,\n",
       "         'brown': 9576,\n",
       "         'green': 9560,\n",
       "         'gray': 9458,\n",
       "         'cyan': 9446,\n",
       "         'red': 9401,\n",
       "         '4': 9016,\n",
       "         '5': 4375,\n",
       "         '6': 1999,\n",
       "         '7': 802,\n",
       "         '8': 307,\n",
       "         '9': 125,\n",
       "         '10': 27})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMEncoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_tokens, hidden_size, n_layers, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(n_tokens, hidden_size)\n",
    "#         dropout = dropout if n_layers > 1 else 0\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "\n",
    "#     def forward(self, x, x_lens):\n",
    "#         x, hidden = self.lstm(self.embedding(x))\n",
    "#         # take the final token hidden state for each item in batch\n",
    "#         return x[torch.arange(x.size(0)),x_lens-1,:] \n",
    "\n",
    "# class VQAModel(nn.Module):\n",
    "\n",
    "#     def __init__(self,\n",
    "#                 num_answers,\n",
    "#                 text_encoder, text_dim,\n",
    "#                 image_encoder, image_dim):\n",
    "#         super().__init__()\n",
    "#         self.text_encoder = text_encoder\n",
    "#         self.image_encoder = image_encoder\n",
    "#         self.classifier = nn.Linear(text_dim + image_dim, num_answers)\n",
    "#         self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "#     def forward(self, text, text_lens, image):\n",
    "#         x_text = text_encoder(text, text_lens)\n",
    "#         x_image = image_encoder(image)\n",
    "#         x = torch.concat((x_text, x_image), axis=1)\n",
    "#         return self.softmax(self.classifier(x))\n",
    "\n",
    "# image_encoder = nn.Sequential(\n",
    "#     models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features,\n",
    "#     nn.AdaptiveAvgPool2d((7, 7)),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(25088, 4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4096, 512)\n",
    "# )\n",
    "                              \n",
    "# text_encoder = LSTMEncoder(len(itos_q), 50, 3)\n",
    "# model = VQAModel(len(itos_a), image_encoder, 512, text_encoder, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(samples):\n",
    "#     tokenized_qs = [list(map(lambda x: stoi_q.get(x, unk_idx), s['question'])) for s in samples]\n",
    "#     question_lens = torch.LongTensor([len(q) for q in tokenized_qs])\n",
    "#     qs_maxlen = question_lens.max()\n",
    "#     tokenized_qs = torch.LongTensor([q + [pad_idx] * (qs_maxlen - len(q)) for q in tokenized_qs])\n",
    "#     images = torch.stack([image_transforms(s['image']) for s in samples])\n",
    "#     answers = torch.LongTensor([stoi_a[s['answer']] for s in samples])\n",
    "#     return tokenized_qs, question_lens, images, answers\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(samples):\n",
    "#     tokenized_qs = [list(map(lambda x: stoi_q.get(x, unk_idx), tokenize(s['question']))) for s in samples]\n",
    "#     question_lens = torch.LongTensor([len(q) for q in tokenized_qs])\n",
    "#     qs_maxlen = question_lens.max()\n",
    "#     tokenized_qs = torch.LongTensor([q + [pad_idx] * (qs_maxlen - len(q)) for q in tokenized_qs])\n",
    "#     images = torch.stack([image_transforms(s['image']) for s in samples])\n",
    "#     answers = torch.LongTensor([stoi_a.get(s['answer'], -1) for s in samples])  # unknown -> -1\n",
    "#     return tokenized_qs, question_lens, images, answers\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "\n",
    "# !ps -aux | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# num_epochs = 5\n",
    "# learning_rate = 0.001\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# n_total_step = len(train_loader)\n",
    "# for epoch in range(num_epochs):\n",
    "#    for i, batch in enumerate(tqdm(train_loader)):\n",
    "#        x_text, x_text_lens, x_img, y = (item.to(device) for item in batch)       \n",
    "#        y_hat = model(x_text, x_text_lens, x_img)\n",
    "#        n_corrects = (y_hat.argmax(axis=1)==y).sum().item()\n",
    "#        loss_value = criterion(y_hat, y)\n",
    "#        loss_value.backward()\n",
    "#        optimizer.step()\n",
    "#        optimizer.zero_grad()\n",
    "#        if (i+1) % 250 == 0:\n",
    "#           print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {loss_value:.5f}, acc = {100*(n_corrects/y.size(0)):.2f}%')\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CLEVRDataset(folder_path, split='val', max_program_length=3)\n",
    "# test_dataset = CLEVRDataset(folder_path, split='val', max_program_length=3)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_accuracy(model, dataloader, device=device):\n",
    "#     model.eval()  \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             tokenized_qs, question_lens, images, answers = batch\n",
    "            \n",
    "#             tokenized_qs = tokenized_qs.to(device)\n",
    "#             question_lens = question_lens.to(device)\n",
    "#             images = images.to(device)\n",
    "#             answers = answers.to(device)\n",
    "\n",
    "#             outputs = model(tokenized_qs, question_lens, images)\n",
    "#             _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "#             correct += (predicted == answers).sum().item()\n",
    "#             total += answers.size(0)\n",
    "            \n",
    "#     accuracy = correct / total\n",
    "#     return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "# test_accuracy = calculate_accuracy(model, test_loader, device=device)\n",
    "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\") # 7.46% -> 2.71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Online Learning with Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_program_vocab(train_dataset):\n",
    "    programs = [item.get('program', []) for item in train_dataset]\n",
    "    \n",
    "    # extract all functions\n",
    "    all_functions = [step['function'] for program in programs for step in program]\n",
    "    function_vocab = Counter(all_functions)\n",
    "\n",
    "    # build index mapping\n",
    "    itos_p = ['<PAD>'] + [func for func, _ in function_vocab.most_common()]\n",
    "    stoi_p = {func: idx for idx, func in enumerate(itos_p)}\n",
    "\n",
    "    return stoi_p, itos_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to Index: {'<PAD>': 0, 'scene': 1, 'count': 2, 'exist': 3, 'filter_color': 4, 'filter_shape': 5, 'filter_material': 6, 'filter_size': 7}\n",
      "Index to String: ['<PAD>', 'scene', 'count', 'exist', 'filter_color', 'filter_shape', 'filter_material', 'filter_size']\n"
     ]
    }
   ],
   "source": [
    "stoi_p, itos_p = build_program_vocab(train_dataset)\n",
    "\n",
    "print(\"String to Index:\", stoi_p)\n",
    "print(\"Index to String:\", itos_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn_1(samples):\n",
    "#     tokenized_qs = [list(map(lambda x: stoi_q.get(x, unk_idx), tokenize(s['question']))) for s in samples]\n",
    "#     question_lens = torch.LongTensor([len(q) for q in tokenized_qs])\n",
    "#     qs_maxlen = question_lens.max()\n",
    "#     tokenized_qs = torch.LongTensor([q + [pad_idx] * (qs_maxlen - len(q)) for q in tokenized_qs])\n",
    "#     images = torch.stack([image_transforms(s['image']) for s in samples])\n",
    "#     answers = torch.LongTensor([stoi_a.get(s['answer'], -1) for s in samples])  # unknown -> -1\n",
    "   \n",
    "#     programs = []\n",
    "#     for s in samples:\n",
    "#         program_indices = torch.LongTensor([\n",
    "#             stoi_p.get(step['function'], -1) for step in s['program']  \n",
    "#         ])\n",
    "#         programs.append(program_indices)\n",
    "\n",
    "#     # padding\n",
    "#     max_program_length = max(len(p) for p in programs)\n",
    "#     padded_programs = torch.stack([\n",
    "#         torch.cat([p, torch.full((max_program_length - len(p),), pad_idx)]) for p in programs\n",
    "#     ])\n",
    "    \n",
    "#     return tokenized_qs, question_lens, images, answers, padded_programs\n",
    "\n",
    "# train_loader_1 = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VQAModelWithProgram(nn.Module):\n",
    "#     def __init__(self, num_answers, image_encoder, image_dim, text_encoder, text_dim, program_vocab_size, program_dim):\n",
    "#         super().__init__()\n",
    "#         self.image_encoder = image_encoder  \n",
    "#         self.text_encoder = text_encoder    \n",
    "#         self.program_encoder = nn.Embedding(program_vocab_size, program_dim)  \n",
    "#         self.classifier = nn.Linear(image_dim + text_dim + program_dim, num_answers)\n",
    "#         self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "#     def forward(self, questions, question_lens, programs, images):\n",
    "\n",
    "#         image_features = self.image_encoder(images)\n",
    "#         question_features = self.text_encoder(questions, question_lens)\n",
    "#         program_features = self.program_encoder(programs).mean(dim=1) \n",
    "\n",
    "#         combined_features = torch.cat((image_features, question_features, program_features), dim=1)\n",
    "\n",
    "#         output = self.classifier(combined_features)\n",
    "#         return self.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def online_learning(model, dataloader, optimizer, criterion, num_iterations=100, device=device):\n",
    "#     performance = [] \n",
    "\n",
    "#     # convert the dataloader into a list of batches\n",
    "#     all_batches = list(dataloader)\n",
    "#     random.shuffle(all_batches) \n",
    "\n",
    "#     for iteration, batch in enumerate(dataloader):\n",
    "#         if iteration >= num_iterations:  # control the number of iterations\n",
    "#             break\n",
    "\n",
    "#         # train on the current batch\n",
    "#         model.train()\n",
    "\n",
    "#         model = model.to(device)\n",
    "#         tokenized_qs, question_lens, images, answers, programs = batch\n",
    "#         tokenized_qs = tokenized_qs.to(device)\n",
    "#         question_lens = question_lens.to(device)\n",
    "#         images = images.to(device)\n",
    "#         answers = answers.to(device)\n",
    "#         programs = programs.to(device)\n",
    "\n",
    "#         outputs = model(tokenized_qs, question_lens, programs, images)\n",
    "#         loss = criterion(outputs, answers)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # use remaining unseen batches for testing\n",
    "#         unseen_batches = all_batches[iteration + 1:]\n",
    "#         if unseen_batches:\n",
    "#             test_batch = unseen_batches[0]\n",
    "#             tokenized_qs, question_lens, images, answers, programs = test_batch\n",
    "            \n",
    "#             tokenized_qs = tokenized_qs.to(device)\n",
    "#             question_lens = question_lens.to(device)\n",
    "#             images = images.to(device)\n",
    "#             answers = answers.to(device)\n",
    "#             programs = programs.to(device)\n",
    "\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(tokenized_qs, question_lens, programs, images)\n",
    "#                 _, predicted = torch.max(outputs, dim=1)\n",
    "#                 test_accuracy = (predicted == answers).float().mean().item()\n",
    "#         else:\n",
    "#             test_accuracy = 0.0  # no more unseen batches\n",
    "\n",
    "#         print(f\"Iteration {iteration+1}/{num_iterations}: Loss = {loss.item():.4f}, Test Accuracy = {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "#     return performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_withprogram = VQAModelWithProgram(len(itos_a), image_encoder, 512, text_encoder, 50, len(ftoi_p), 50)\n",
    "# performance = online_learning(model_withprogram, train_loader_1, optimizer, criterion, num_iterations=25, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize model with dynamic fusion\n",
    "# model_with_dynamic_fusion = VQAModelWithDynamicFusion(\n",
    "#     num_answers=len(itos_a),\n",
    "#     image_encoder=image_encoder,\n",
    "#     image_dim=512,\n",
    "#     text_encoder=text_encoder,\n",
    "#     text_dim=50,\n",
    "#     program_vocab_size=len(itos_p),\n",
    "#     program_dim=50,\n",
    "#     hidden_dim=512\n",
    "# )\n",
    "\n",
    "# # Train with online learning\n",
    "# performance = online_learning(\n",
    "#     model_with_dynamic_fusion,\n",
    "#     train_loader_1,\n",
    "#     optimizer,\n",
    "#     criterion,\n",
    "#     num_iterations=25,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the model's predictions are basically random..try to use dynamic fusion weights rather than simple concatenation? How to make the approach more \"interactive\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upgrade structure \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Image Encoder：VGG16 -> ResNet\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # remove last layer\n",
    "        self.proj = nn.Linear(2048, 512)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "# Text Encoder: LSTM -> BERT\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.proj = nn.Linear(768, 512)  \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "        x = self.proj(pooled_output)\n",
    "        return x\n",
    "\n",
    "# Program Encoder: LSTM\n",
    "class ProgramEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(ProgramEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.proj = nn.Linear(hidden_dim, 512)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = h_n[-1]  # last hidden state\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "# Initialization\n",
    "image_encoder = ImageEncoder()\n",
    "text_encoder = TextEncoder()\n",
    "program_encoder = ProgramEncoder(vocab_size=len(itos_p))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define baseline model class (without program)\n",
    "class BaselineVQAModel(nn.Module):\n",
    "    def __init__(self, num_answers, image_encoder, text_encoder):\n",
    "        super(BaselineVQAModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.classifier = nn.Linear(512 + 512, num_answers) \n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(image)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        combined_features = torch.cat([image_features, text_features], dim=1)\n",
    "        output = self.classifier(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, input_ids, attention_mask, answers = batch\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        answers = answers.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, input_ids, attention_mask, answers = batch \n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            answers = answers.to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, answers)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def collate_fn(samples):\n",
    "    # extract necessary attributes\n",
    "    questions = [s['question'] for s in samples]\n",
    "    answers = [s['answer'] for s in samples]\n",
    "    images = [s['image'] for s in samples]\n",
    "    \n",
    "    # Use BERT tokenizer to process questions\n",
    "    tokenized = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = tokenized['input_ids']\n",
    "    attention_mask = tokenized['attention_mask']\n",
    "\n",
    "    # process images\n",
    "    images = torch.stack([image_transforms(img) for img in images])\n",
    "\n",
    "    # process answers\n",
    "    answers = torch.tensor([stoi_a.get(ans, -1) for ans in answers])  # unknown -> -1\n",
    "\n",
    "    return images, input_ids, attention_mask, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.8594, Train Acc: 73.26%\n",
      "Val Loss: 1.9207, Val Acc: 47.82%\n",
      "Epoch 2/3\n",
      "Train Loss: 0.4619, Train Acc: 83.75%\n",
      "Val Loss: 2.3756, Val Acc: 45.15%\n",
      "Epoch 3/3\n",
      "Train Loss: 0.3264, Train Acc: 89.10%\n",
      "Val Loss: 2.4332, Val Acc: 48.79%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "baseline_model = BaselineVQAModel(\n",
    "    num_answers=len(itos_a),\n",
    "    image_encoder=image_encoder,\n",
    "    text_encoder=text_encoder\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(baseline_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(baseline_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(baseline_model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train answers distribution: Counter({'yes': 764, '2': 308, '1': 273, '3': 207, '4': 128, 'no': 75, '5': 48, '6': 44, '0': 42, '7': 34, '8': 15, '9': 13, '10': 12})\n",
      "Val answers distribution: Counter({'yes': 130, '2': 75, '3': 60, '1': 55, 'no': 20, '4': 19, '5': 16, '0': 13, '7': 10, '6': 8, '8': 3, '10': 2, '9': 1})\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "# train_answers = [item['answer'] for item in train_dataset]\n",
    "# val_answers = [item['answer'] for item in val_dataset]\n",
    "\n",
    "# print(\"Train answers distribution:\", Counter(train_answers))\n",
    "# print(\"Val answers distribution:\", Counter(val_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Fusion: Multihead Attention\n",
    "class DynamicFusion(nn.Module):\n",
    "    def __init__(self, feature_dim=512, num_heads=8):\n",
    "        super(DynamicFusion, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.proj = nn.Linear(feature_dim * 3, feature_dim) \n",
    "\n",
    "    def forward(self, image_features, text_features, program_features):\n",
    "        # concat\n",
    "        combined_features = torch.cat([image_features.unsqueeze(1),\n",
    "                                       text_features.unsqueeze(1),\n",
    "                                       program_features.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # multihead attention\n",
    "        attn_output, _ = self.multihead_attn(combined_features, combined_features, combined_features)\n",
    "        attn_output = attn_output.mean(dim=1)  # mean attention\n",
    "\n",
    "        # fusion features\n",
    "        fusion_features = self.proj(torch.cat([image_features, text_features, program_features], dim=1))\n",
    "        return fusion_features\n",
    "\n",
    "\n",
    "\n",
    "# Full Model\n",
    "class VQAModelWithDynamicFusion(nn.Module):\n",
    "    def __init__(self, num_answers, image_encoder, text_encoder, program_encoder, fusion_module):\n",
    "        super(VQAModelWithDynamicFusion, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.program_encoder = program_encoder\n",
    "        self.fusion_module = fusion_module\n",
    "        self.classifier = nn.Linear(512, num_answers)  \n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask, program):\n",
    "        image_features = self.image_encoder(image)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        program_features = self.program_encoder(program)\n",
    "        fusion_features = self.fusion_module(image_features, text_features, program_features)\n",
    "        output = self.classifier(fusion_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "fusion_module = DynamicFusion()\n",
    "model = VQAModelWithDynamicFusion(\n",
    "    num_answers=len(itos_a), \n",
    "    image_encoder=image_encoder,\n",
    "    text_encoder=text_encoder,\n",
    "    program_encoder=program_encoder,\n",
    "    fusion_module=fusion_module\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_1(samples):\n",
    "    # extract necessary attributes\n",
    "    questions = [s['question'] for s in samples]\n",
    "    answers = [s['answer'] for s in samples]\n",
    "    programs = [s['program'] for s in samples]  \n",
    "    images = [s['image'] for s in samples]\n",
    "    \n",
    "    # Use BERT tokenizer to process questions\n",
    "    tokenized = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = tokenized['input_ids']\n",
    "    attention_mask = tokenized['attention_mask']\n",
    "\n",
    "    # process images\n",
    "    images = torch.stack([image_transforms(img) for img in images])\n",
    "\n",
    "    # process programs\n",
    "    max_program_length = max(max(len(p) for p in programs), 1)  \n",
    "    padded_programs = []\n",
    "    for program in programs:\n",
    "        if len(program) == 0:\n",
    "            program_indices = [0]  # index of <PAD> \n",
    "        else:\n",
    "            # function -> index\n",
    "            program_indices = [stoi_p.get(step['function'], 0) for step in program]\n",
    "        # padding\n",
    "        padded_program = program_indices + [0] * (max_program_length - len(program_indices))\n",
    "        padded_programs.append(torch.tensor(padded_program, dtype=torch.long))\n",
    "    padded_programs = torch.stack(padded_programs)\n",
    "\n",
    "    # process answers\n",
    "    answers = torch.tensor([stoi_a.get(ans, -1) for ans in answers])  # unknown -> -1\n",
    "\n",
    "    return images, input_ids, attention_mask, padded_programs, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_1 = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn_1)\n",
    "val_loader_1 = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, input_ids, attention_mask, programs, answers = batch\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        programs = programs.to(device)\n",
    "        answers = answers.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask, programs)\n",
    "        loss = criterion(outputs, answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_1(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, input_ids, attention_mask, programs, answers = batch\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            programs = programs.to(device)\n",
    "            answers = answers.to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask, programs)\n",
    "            loss = criterion(outputs, answers)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.2763, Train Acc: 91.65%\n",
      "Val Loss: 3.6969, Val Acc: 43.45%\n",
      "Epoch 2/3\n",
      "Train Loss: 0.1345, Train Acc: 95.77%\n",
      "Val Loss: 3.4672, Val Acc: 47.57%\n",
      "Epoch 3/3\n",
      "Train Loss: 0.1020, Train Acc: 97.10%\n",
      "Val Loss: 3.4026, Val Acc: 43.20%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_1(model, train_loader_1, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate_1(model, val_loader_1, criterion, device)\n",
    "    scheduler.step(val_loss)  # Adjust the learning rate based on the validation set loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def online_learning(model, dataloader, optimizer, criterion, num_iterations=100, device=device):\n",
    "    performance = []\n",
    "\n",
    "    all_batches = list(dataloader)\n",
    "    random.shuffle(all_batches)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # current batch\n",
    "        batch = all_batches[iteration % len(all_batches)]\n",
    "\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "\n",
    "        images, input_ids, attention_mask, programs, answers = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        images = images.to(device)\n",
    "        answers = answers.to(device)\n",
    "        programs = programs.to(device)\n",
    "\n",
    "        outputs = model(images, input_ids, attention_mask, programs)\n",
    "        loss = criterion(outputs, answers)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # use remaining unseen batches for testing\n",
    "        unseen_batches = all_batches[iteration + 1:]\n",
    "        if unseen_batches:\n",
    "            test_batch = unseen_batches[0]\n",
    "            images_test, input_ids_test, attention_mask_test, programs_test, answers_test = test_batch\n",
    "\n",
    "            input_ids_test = input_ids_test.to(device)\n",
    "            attention_mask_test = attention_mask_test.to(device)\n",
    "            images_test = images_test.to(device)\n",
    "            answers_test = answers_test.to(device)\n",
    "            programs_test = programs_test.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs_test = model(images_test, input_ids_test, attention_mask_test, programs_test)\n",
    "                _, predicted = torch.max(outputs_test, dim=1)\n",
    "                test_accuracy = (predicted == answers_test).float().mean().item()\n",
    "        else:\n",
    "            test_accuracy = 0.0  \n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}: Loss = {loss.item():.4f}, Test Accuracy = {test_accuracy * 100:.2f}%\")\n",
    "        performance.append((loss.item(), test_accuracy))\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/25: Loss = 3.7145, Test Accuracy = 0.00%\n",
      "Iteration 2/25: Loss = 3.5823, Test Accuracy = 0.00%\n",
      "Iteration 3/25: Loss = 3.4050, Test Accuracy = 8.33%\n",
      "Iteration 4/25: Loss = 3.2283, Test Accuracy = 37.50%\n",
      "Iteration 5/25: Loss = 2.8971, Test Accuracy = 50.00%\n",
      "Iteration 6/25: Loss = 2.6252, Test Accuracy = 43.75%\n",
      "Iteration 7/25: Loss = 2.6083, Test Accuracy = 37.50%\n",
      "Iteration 8/25: Loss = 2.4597, Test Accuracy = 37.50%\n",
      "Iteration 9/25: Loss = 2.4227, Test Accuracy = 25.00%\n",
      "Iteration 10/25: Loss = 2.2773, Test Accuracy = 31.25%\n",
      "Iteration 11/25: Loss = 2.3994, Test Accuracy = 43.75%\n",
      "Iteration 12/25: Loss = 2.4256, Test Accuracy = 43.75%\n",
      "Iteration 13/25: Loss = 1.9384, Test Accuracy = 50.00%\n",
      "Iteration 14/25: Loss = 1.9498, Test Accuracy = 43.75%\n",
      "Iteration 15/25: Loss = 1.4540, Test Accuracy = 43.75%\n",
      "Iteration 16/25: Loss = 1.6086, Test Accuracy = 62.50%\n",
      "Iteration 17/25: Loss = 1.9694, Test Accuracy = 31.25%\n",
      "Iteration 18/25: Loss = 1.9138, Test Accuracy = 62.50%\n",
      "Iteration 19/25: Loss = 1.5953, Test Accuracy = 56.25%\n",
      "Iteration 20/25: Loss = 1.7223, Test Accuracy = 37.50%\n",
      "Iteration 21/25: Loss = 1.7544, Test Accuracy = 43.75%\n",
      "Iteration 22/25: Loss = 1.6141, Test Accuracy = 50.00%\n",
      "Iteration 23/25: Loss = 1.9891, Test Accuracy = 56.25%\n",
      "Iteration 24/25: Loss = 1.6043, Test Accuracy = 62.50%\n",
      "Iteration 25/25: Loss = 1.6354, Test Accuracy = 50.00%\n"
     ]
    }
   ],
   "source": [
    "performance = online_learning(model, val_loader_1, optimizer, criterion, num_iterations=25, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
