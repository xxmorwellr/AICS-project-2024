{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Enhancing Visual Reasoning in VQA Tasks through Interactive Learning\n",
    "\n",
    "For my course project, I want to explore if interactive learning can improve spatial reasoning in Visual Question Answering (VQA) tasks.\n",
    "\n",
    "## Background\n",
    "\n",
    "From our readings and discussions so far, one of the key observations I've made is that current neural network models struggle with tasks requiring spatial reasoning or counting. They often rely on **pattern recognition** rather than true spatial understanding. Given this, I am somewhat skeptical of the research path based on the inherent, restrictive approach and am more drawn to exploring **interactive learning**. Can interactive learning bridge this gap, or bring more insights to traditional models?\n",
    "\n",
    "## Available Resources\n",
    "\n",
    "**Dataset**: CLEVR, a diagnostic dataset for testing spatial and logical reasoning. It also provides scene graphs as a foundation of interactive learning.  \n",
    "**Existing Baselines**: Neural network-based VQA models (e.g., CNN + LSTM).\n",
    "\n",
    "## Current Approaches\n",
    "\n",
    "1. Implement a baseline VQA model to evaluate counting and spatial reasoning tasks. \n",
    "2. Introduce online learning approach, using given answers and programs to iteratively improve model predictions.\n",
    "Compare model performance across these approaches and analyze their reasoning capabilities.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "Improved accuracy on counting and spatial reasoning tasks through interaction.\n",
    "Insights into the potential interactive methods for improving static learning in VQA systems.\n",
    "\n",
    "CLEVR paper: https://arxiv.org/pdf/1612.06890\n",
    "\n",
    "dataset: https://github.com/facebookresearch/clevr-dataset-gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Data\n",
    "In preliminary practice, I control the length of program to limit the complexity of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/srv/data/CLEVR_v1.0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRDataset(Dataset):\n",
    "    def __init__(self, corpus_dir, split='train', transform=None, max_program_length=None):\n",
    "        \"\"\"\n",
    "        Initialize CLEVR dataset\n",
    "        :param corpus_dir: root directory path of CLEVR dataset\n",
    "        :param split: data split ('train', 'val', 'test')\n",
    "        :param transform: image preprocessing transform\n",
    "        :param max_program_length: maximum length of program, subset questions\n",
    "        \"\"\"\n",
    "        self.path = Path(corpus_dir)\n",
    "        self.split = split\n",
    "        # self.mode = mode  # 'train' or 'test'\n",
    "        # self.feedback_mode = feedback_mode  # 'none', 'answer', 'program'\n",
    "        self.transform = transform \n",
    "        self.max_program_length = max_program_length\n",
    "\n",
    "        questions_file = self.path / f'questions/CLEVR_{self.split}_questions.json'\n",
    "        all_items = json.load(questions_file.open())['questions']\n",
    "\n",
    "        # subset\n",
    "        if self.max_program_length is not None:\n",
    "            self.items = [\n",
    "                item for item in all_items if len(item.get('program', [])) <= self.max_program_length\n",
    "            ]\n",
    "        else:\n",
    "            self.items = all_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "\n",
    "        # load image\n",
    "        img_path = self.path / 'images' / self.split / item['image_filename']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # basic data\n",
    "        data = {\n",
    "            'image': image, \n",
    "            'question': item['question'], \n",
    "            'answer': item.get('answer', '<NO_ANS>'), \n",
    "            'program': item.get('program', [])  # optional\n",
    "        }\n",
    "\n",
    "        # # add feedback \n",
    "        # if self.mode == 'train' and self.feedback_mode != 'none':\n",
    "        #     if self.feedback_mode == 'answer':\n",
    "        #         data['feedback'] = {'answer': item.get('answer', '<NO_ANS>')}\n",
    "        #     elif self.feedback_mode == 'program':\n",
    "        #         data['feedback'] = {'program': item.get('program', [])}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load scene graphs\n",
    "\n",
    "# scene_path = folder_path + \"/scenes/CLEVR_val_scenes.json\"\n",
    "# with open(scene_path, 'r') as f:\n",
    "#     scenes = json.load(f)\n",
    "\n",
    "# # look into the structure\n",
    "# print(scenes['scenes'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load questions\n",
    "\n",
    "# questions_path = folder_path + \"/questions/CLEVR_val_questions.json\"\n",
    "# with open(questions_path, 'r') as f:\n",
    "#     questions = json.load(f)\n",
    "\n",
    "# # # look into the structure\n",
    "# # print(questions['questions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect simple 'count' questions as a starting point\n",
    "# def extract_count_questions(questions_data, max_steps=3):\n",
    "#     count_questions = []\n",
    "#     for question in questions_data['questions']:\n",
    "#         program = question['program']\n",
    "#         if any(step['function'] == 'count' for step in program) and len(program) <= max_steps:\n",
    "#             count_questions.append({\n",
    "#                 'question_index': question['question_index'],\n",
    "#                 'image_index': question['image_index'],\n",
    "#                 'image_filename': question['image_filename'],\n",
    "#                 'question': question['question'],\n",
    "#                 'answer': question['answer'],\n",
    "#                 'program': question['program']\n",
    "#             })\n",
    "#     return count_questions\n",
    "\n",
    "# count_questions = extract_count_questions(questions, max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Total 'count' questions found: {len(count_questions)}\")\n",
    "# print(\"Sample 'count' question:\", count_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate the problem and analyze its program\n",
    "# for question in count_questions:\n",
    "#     print(\"Question:\", question['question'])\n",
    "#     print(\"Answer:\", question['answer'])\n",
    "#     print(\"Program:\")\n",
    "    \n",
    "#     for step in question['program']:\n",
    "#         print(f\"  Function: {step['function']}, Inputs: {step['inputs']}, Values: {step['value_inputs']}\")\n",
    "#     print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(count_questions)\n",
    "\n",
    "# # Split data for initialization and testing\n",
    "# train_questions = count_questions[:200]\n",
    "# test_questions = count_questions[200:262]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline VQA Model\n",
    "\n",
    "In previous lecture, Bill introduced a base VQA model, which utilized LSTM model to process question and VGG16 to extract image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CLEVRDataset(folder_path, split='val', max_program_length=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a global vocab\n",
    "def load_all_questions(folder_path):\n",
    "    all_items = []\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        questions_file = Path(folder_path) / f'questions/CLEVR_{split}_questions.json'\n",
    "        all_items.extend(json.load(questions_file.open())['questions'])\n",
    "    return all_items\n",
    "\n",
    "tokenize = lambda text: re.findall(r'\\w+|\\S', text.lower())\n",
    "def build_global_vocab(folder_path):\n",
    "    all_items = load_all_questions(folder_path)\n",
    "\n",
    "    # debug\n",
    "    for item in all_items:\n",
    "        if 'answer' not in item:\n",
    "            print(\"Item without 'answer' key:\", item)\n",
    "            break\n",
    "            \n",
    "    all_questions = [tokenize(item['question']) for item in all_items if 'question' in item]\n",
    "    all_answers = [item.get('answer', '<NO_ANS>') for item in all_items]\n",
    "\n",
    "    question_vocab = Counter([token for question in all_questions for token in question])\n",
    "    answer_vocab = Counter(all_answers)\n",
    "\n",
    "    itos_q = ['<PAD>', '<UNK>'] + [s[0] for s in question_vocab.most_common()]\n",
    "    stoi_q = {s: i for i, s in enumerate(itos_q)}\n",
    "    pad_idx, unk_idx = stoi_q['<PAD>'], stoi_q['<UNK>']\n",
    "\n",
    "    itos_a = ['<NO_ANS>'] + [s[0] for s in answer_vocab.most_common()]\n",
    "    stoi_a = {s: i for i, s in enumerate(itos_a)}\n",
    "\n",
    "    return question_vocab, answer_vocab, stoi_q, itos_q, stoi_a, itos_a, pad_idx, unk_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item without 'answer' key: {'image_index': 0, 'split': 'test', 'image_filename': 'CLEVR_test_000000.png', 'question_index': 0, 'question': 'Is there anything else that is the same shape as the small brown matte object?'}\n"
     ]
    }
   ],
   "source": [
    "question_vocab, answer_vocab, stoi_q, itos_q, stoi_a, itos_a, pad_idx, unk_idx = build_global_vocab(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_tokens, hidden_size, n_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, hidden_size)\n",
    "        dropout = dropout if n_layers > 1 else 0\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, x_lens):\n",
    "        x, hidden = self.lstm(self.embedding(x))\n",
    "        # take the final token hidden state for each item in batch\n",
    "        return x[torch.arange(x.size(0)),x_lens-1,:] \n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                num_answers,\n",
    "                text_encoder, text_dim,\n",
    "                image_encoder, image_dim):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.classifier = nn.Linear(text_dim + image_dim, num_answers)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, text, text_lens, image):\n",
    "        x_text = text_encoder(text, text_lens)\n",
    "        x_image = image_encoder(image)\n",
    "        x = torch.concat((x_text, x_image), axis=1)\n",
    "        return self.softmax(self.classifier(x))\n",
    "\n",
    "image_encoder = nn.Sequential(\n",
    "    models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features,\n",
    "    nn.AdaptiveAvgPool2d((7, 7)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(25088, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4096, 512)\n",
    ")\n",
    "                              \n",
    "text_encoder = LSTMEncoder(len(itos_q), 50, 3)\n",
    "model = VQAModel(len(itos_a), image_encoder, 512, text_encoder, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(samples):\n",
    "#     tokenized_qs = [list(map(lambda x: stoi_q.get(x, unk_idx), s['question'])) for s in samples]\n",
    "#     question_lens = torch.LongTensor([len(q) for q in tokenized_qs])\n",
    "#     qs_maxlen = question_lens.max()\n",
    "#     tokenized_qs = torch.LongTensor([q + [pad_idx] * (qs_maxlen - len(q)) for q in tokenized_qs])\n",
    "#     images = torch.stack([image_transforms(s['image']) for s in samples])\n",
    "#     answers = torch.LongTensor([stoi_a[s['answer']] for s in samples])\n",
    "#     return tokenized_qs, question_lens, images, answers\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    tokenized_qs = [list(map(lambda x: stoi_q.get(x, unk_idx), tokenize(s['question']))) for s in samples]\n",
    "    question_lens = torch.LongTensor([len(q) for q in tokenized_qs])\n",
    "    qs_maxlen = question_lens.max()\n",
    "    tokenized_qs = torch.LongTensor([q + [pad_idx] * (qs_maxlen - len(q)) for q in tokenized_qs])\n",
    "    images = torch.stack([image_transforms(s['image']) for s in samples])\n",
    "    answers = torch.LongTensor([stoi_a.get(s['answer'], -1) for s in samples])  # unknown -> -1\n",
    "    return tokenized_qs, question_lens, images, answers\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "\n",
    "# !ps -aux | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c459c286d0485cb45f5183894d9a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091be3f71016401bba90edc5e2466b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b3e820160848dcb3623585189762de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6563c103b9554e7b8e10eed1e27dea0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca100b4a0f274daeb47731e5ed954a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "n_total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "   for i, batch in enumerate(tqdm(train_loader)):\n",
    "       x_text, x_text_lens, x_img, y = (item.to(device) for item in batch)       \n",
    "       y_hat = model(x_text, x_text_lens, x_img)\n",
    "       n_corrects = (y_hat.argmax(axis=1)==y).sum().item()\n",
    "       loss_value = criterion(y_hat, y)\n",
    "       loss_value.backward()\n",
    "       optimizer.step()\n",
    "       optimizer.zero_grad()\n",
    "       if (i+1) % 250 == 0:\n",
    "          print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {loss_value:.5f}, acc = {100*(n_corrects/y.size(0)):.2f}%')\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CLEVRDataset(folder_path, split='test', max_program_length=3)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device=device):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            tokenized_qs, question_lens, images, answers = batch\n",
    "            \n",
    "            tokenized_qs = tokenized_qs.to(device)\n",
    "            question_lens = question_lens.to(device)\n",
    "            images = images.to(device)\n",
    "            answers = answers.to(device)\n",
    "\n",
    "            outputs = model(tokenized_qs, question_lens, images)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            correct += (predicted == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 2.08%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device=device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\") # 7.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Online Learning with Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_program_vocab(train_dataset):\n",
    "    programs = [item.get('program', []) for item in train_dataset]\n",
    "    \n",
    "    # extract all functions\n",
    "    all_functions = [step['function'] for program in programs for step in program]\n",
    "    function_vocab = Counter(all_functions)\n",
    "\n",
    "    # build index mapping\n",
    "    itos_p = ['<PAD>'] + [func for func, _ in function_vocab.most_common()]\n",
    "    stoi_p = {func: idx for idx, func in enumerate(itof_p)}\n",
    "\n",
    "    return ftoi_p, itof_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to Index: {'<PAD>': 0, 'scene': 1, 'count': 2, 'filter_color': 3, 'exist': 4, 'filter_shape': 5, 'filter_material': 6, 'filter_size': 7}\n",
      "Index to String: ['<PAD>', 'scene', 'count', 'filter_color', 'exist', 'filter_shape', 'filter_material', 'filter_size']\n"
     ]
    }
   ],
   "source": [
    "stoi_p, itos_p = build_program_vocab(train_dataset)\n",
    "\n",
    "print(\"String to Index:\", stoi_p)\n",
    "print(\"Index to String:\", itos_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_1(samples):\n",
    "    tokenized_qs = [list(map(lambda x: stoi_q.get(x, unk_idx), tokenize(s['question']))) for s in samples]\n",
    "    question_lens = torch.LongTensor([len(q) for q in tokenized_qs])\n",
    "    qs_maxlen = question_lens.max()\n",
    "    tokenized_qs = torch.LongTensor([q + [pad_idx] * (qs_maxlen - len(q)) for q in tokenized_qs])\n",
    "    images = torch.stack([image_transforms(s['image']) for s in samples])\n",
    "    answers = torch.LongTensor([stoi_a.get(s['answer'], -1) for s in samples])  # unknown -> -1\n",
    "   \n",
    "    programs = []\n",
    "    for s in samples:\n",
    "        program_indices = torch.LongTensor([\n",
    "            stoi_p.get(step['function'], -1) for step in s['program']  \n",
    "        ])\n",
    "        programs.append(program_indices)\n",
    "\n",
    "    # padding\n",
    "    max_program_length = max(len(p) for p in programs)\n",
    "    padded_programs = torch.stack([\n",
    "        torch.cat([p, torch.full((max_program_length - len(p),), pad_idx)]) for p in programs\n",
    "    ])\n",
    "    \n",
    "    return tokenized_qs, question_lens, images, answers, padded_programs\n",
    "\n",
    "train_loader_1 = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModelWithProgram(nn.Module):\n",
    "    def __init__(self, num_answers, image_encoder, image_dim, text_encoder, text_dim, program_vocab_size, program_dim):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder  \n",
    "        self.text_encoder = text_encoder    \n",
    "        self.program_encoder = nn.Embedding(program_vocab_size, program_dim)  \n",
    "        self.classifier = nn.Linear(image_dim + text_dim + program_dim, num_answers)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, questions, question_lens, programs, images):\n",
    "\n",
    "        image_features = self.image_encoder(images)\n",
    "        question_features = self.text_encoder(questions, question_lens)\n",
    "        program_features = self.program_encoder(programs).mean(dim=1) \n",
    "\n",
    "        combined_features = torch.cat((image_features, question_features, program_features), dim=1)\n",
    "\n",
    "        output = self.classifier(combined_features)\n",
    "        return self.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def online_learning(model, dataloader, optimizer, criterion, num_iterations=100, device=device):\n",
    "    performance = [] \n",
    "\n",
    "    # convert the dataloader into a list of batches\n",
    "    all_batches = list(dataloader)\n",
    "    random.shuffle(all_batches) \n",
    "\n",
    "    for iteration, batch in enumerate(dataloader):\n",
    "        if iteration >= num_iterations:  # control the number of iterations\n",
    "            break\n",
    "\n",
    "        # train on the current batch\n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        tokenized_qs, question_lens, images, answers, programs = batch\n",
    "        tokenized_qs = tokenized_qs.to(device)\n",
    "        question_lens = question_lens.to(device)\n",
    "        images = images.to(device)\n",
    "        answers = answers.to(device)\n",
    "        programs = programs.to(device)\n",
    "\n",
    "        outputs = model(tokenized_qs, question_lens, programs, images)\n",
    "        loss = criterion(outputs, answers)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # use remaining unseen batches for testing\n",
    "        unseen_batches = all_batches[iteration + 1:]\n",
    "        if unseen_batches:\n",
    "            test_batch = unseen_batches[0]\n",
    "            tokenized_qs, question_lens, images, answers, programs = test_batch\n",
    "            \n",
    "            tokenized_qs = tokenized_qs.to(device)\n",
    "            question_lens = question_lens.to(device)\n",
    "            images = images.to(device)\n",
    "            answers = answers.to(device)\n",
    "            programs = programs.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokenized_qs, question_lens, programs, images)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                test_accuracy = (predicted == answers).float().mean().item()\n",
    "        else:\n",
    "            test_accuracy = 0.0  # no more unseen batches\n",
    "\n",
    "        print(f\"Iteration {iteration+1}/{num_iterations}: Loss = {loss.item():.4f}, Test Accuracy = {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_withprogram = VQAModelWithProgram(len(itos_a), image_encoder, 512, text_encoder, 50, len(ftoi_p), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/25: Loss = 3.4026, Test Accuracy = 0.00%\n",
      "Iteration 2/25: Loss = 3.4032, Test Accuracy = 0.00%\n",
      "Iteration 3/25: Loss = 3.3998, Test Accuracy = 6.25%\n",
      "Iteration 4/25: Loss = 3.4023, Test Accuracy = 0.00%\n",
      "Iteration 5/25: Loss = 3.4005, Test Accuracy = 0.00%\n",
      "Iteration 6/25: Loss = 3.4014, Test Accuracy = 0.00%\n",
      "Iteration 7/25: Loss = 3.4009, Test Accuracy = 0.00%\n",
      "Iteration 8/25: Loss = 3.4040, Test Accuracy = 0.00%\n",
      "Iteration 9/25: Loss = 3.4008, Test Accuracy = 0.00%\n",
      "Iteration 10/25: Loss = 3.4012, Test Accuracy = 0.00%\n",
      "Iteration 11/25: Loss = 3.4011, Test Accuracy = 12.50%\n",
      "Iteration 12/25: Loss = 3.4028, Test Accuracy = 6.25%\n",
      "Iteration 13/25: Loss = 3.4007, Test Accuracy = 12.50%\n",
      "Iteration 14/25: Loss = 3.4005, Test Accuracy = 0.00%\n",
      "Iteration 15/25: Loss = 3.4003, Test Accuracy = 6.25%\n",
      "Iteration 16/25: Loss = 3.4022, Test Accuracy = 6.25%\n",
      "Iteration 17/25: Loss = 3.4044, Test Accuracy = 6.25%\n",
      "Iteration 18/25: Loss = 3.4017, Test Accuracy = 0.00%\n",
      "Iteration 19/25: Loss = 3.4036, Test Accuracy = 0.00%\n",
      "Iteration 20/25: Loss = 3.4036, Test Accuracy = 6.25%\n",
      "Iteration 21/25: Loss = 3.4007, Test Accuracy = 0.00%\n",
      "Iteration 22/25: Loss = 3.4030, Test Accuracy = 6.25%\n",
      "Iteration 23/25: Loss = 3.4011, Test Accuracy = 0.00%\n",
      "Iteration 24/25: Loss = 3.4013, Test Accuracy = 6.25%\n",
      "Iteration 25/25: Loss = 3.4013, Test Accuracy = 6.25%\n"
     ]
    }
   ],
   "source": [
    "performance = online_learning(model_withprogram, train_loader_1, optimizer, criterion, num_iterations=25, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the model's predictions are basically random..try to use dynamic fusion weights rather than simple concatenation? How to make the approach more \"interactive\"?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
